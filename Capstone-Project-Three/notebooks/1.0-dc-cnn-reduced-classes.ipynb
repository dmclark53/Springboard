{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "right-willow",
   "metadata": {},
   "source": [
    "# Deep Learning Model: CNN - Reduced Classes\n",
    "## Business Problem\n",
    "Leukemia is a type of cancer of the blood that often affects young people. In the past, pathologists would diagnose patients by eye after examining blood smear images under the microscope. But, this is time consuming and tedious. Advances in image recognition technology have come a long ways since their inception. Therefore, automated solutions using computers would be of great benefit to the medical community to aid in cancer diagnoses.\n",
    "\n",
    "The goal of this project is to address the following question: How can the doctorâ€™s at the Munich University Hospital automate the diagnosis of patients with leukemia using images from blood smears?\n",
    "\n",
    "## Approach\n",
    "This notebook will use the previously built model, but only a subset of the training data the includes just a binary class. From this data, I will be able to assess whether this model has difficulty with the large class imbalance between all 15 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from time import time\n",
    "\n",
    "from keras import layers\n",
    "from keras import metrics\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from src.data_setup import make_dataset as md\n",
    "from src.modeling import evaluate_model as em\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-cathedral",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the pickled training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = md.load_train_test('gray_rescale12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-flooring",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Unflatten\n",
    "Unflatten the feature arrays, converting them back into arrays of 2-dimensional images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten(X):\n",
    "    dimension = int(np.sqrt(X.shape[1]))\n",
    "    return X.reshape((len(X), dimension, dimension, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unflatten = unflatten(X_train)\n",
    "X_train_unflatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_unflatten = unflatten(X_test)\n",
    "X_test_unflatten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-request",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "Normalize the features, to values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The maximum value for the training set is {X_train_unflatten.max()}.')\n",
    "print(f'The maximum value for the test set is {X_test_unflatten.max()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = X_train_unflatten / X_train_unflatten.max()\n",
    "X_test_normalized = X_test_unflatten / X_test_unflatten.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The maximum value for the normalized training set is {X_train_normalized.max()}.')\n",
    "print(f'The maximum value for the normalized test set is {X_test_normalized.max()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-warrant",
   "metadata": {},
   "source": [
    "### Categories\n",
    "First, encode the labels to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encodings = {value: i for i, value in enumerate(np.unique(y_train))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = pd.Series(y_train).replace(label_encodings).values\n",
    "y_test_encoded = pd.Series(y_test).replace(label_encodings).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-event",
   "metadata": {},
   "source": [
    "Second, encode the integer labels as one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_one_hot[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-prerequisite",
   "metadata": {},
   "source": [
    "## Validation Set\n",
    "Now that we have preprocessed the training data, I will create a validation set. This will be used to evaluate how the deep learning model is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized, X_val, y_train_one_hot, y_val = train_test_split(X_train_normalized, y_train_one_hot, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-bacon",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_unflatten.shape[1:]\n",
    "print(f'The input shape is {input_shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = models.Sequential([\n",
    "    layers.Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Conv2D(16, kernel_size=(7, 7), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(600, activation='relu'),\n",
    "    layers.Dense(150, activation='relu'),\n",
    "    layers.Dense(38, activation='relu'),\n",
    "    layers.Dense(15, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-friendship",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\n",
    "                  metrics.Accuracy(),\n",
    "                  metrics.categorical_accuracy,\n",
    "                  metrics.Precision(),\n",
    "                  metrics.Recall()\n",
    "              ])\n",
    "results_1 = model_1.fit(X_train_normalized, y_train_one_hot, validation_data=(X_val, y_val), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-money",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "Make class predictions using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_cnn = model_1.predict(X_train_normalized)\n",
    "y_pred_cnn = model_1.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-bubble",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_train_val_losses(results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_train_one_hot, axis=1), np.argmax(y_pred_train_cnn, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(np.argmax(y_test_one_hot, axis=1), np.argmax(y_pred_cnn, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_confusion_matrix(y_test_one_hot, y_pred_cnn, label_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-window",
   "metadata": {},
   "source": [
    "### Train Model - Use Weighted Classes\n",
    "To counter the class imbalance, I will try weighting the classes by importance. More weight will be given to the classes with less representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "class_weights_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[\n",
    "                    metrics.Accuracy(),\n",
    "                    metrics.categorical_accuracy,\n",
    "                    metrics.Precision(),\n",
    "                    metrics.Recall()\n",
    "                ])\n",
    "results_2 = model_1.fit(X_train_normalized, y_train_one_hot, validation_data=(X_val, y_val), epochs=50, batch_size=64, class_weight=class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot_train_val_losses(results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-indication",
   "metadata": {},
   "source": [
    "**Observations:** The chaotic loss indicates that the model is unable to learn anything useful from the training data.\n",
    "\n",
    "## Summary\n",
    "I created a deepling model using a convolutional neural network (CNN) to predict the 15 different classes of leukocite. The model used weighted classes to counter class imbalance. The dataset of images was rescaled by 12% and converted to grayscale.\n",
    "\n",
    "After examining the training performance by comparing the validation and training loss over epoch, I determined that the model is having difficulty learning anything useful from the data. There are several factors that could contribute to this poor model performance. A few factors include:\n",
    "\n",
    "1. Class imbalance issues.\n",
    "2. Insufficient features due to rescaled images.\n",
    "3. Wrong model architecture.\n",
    "\n",
    "## Future Direction\n",
    "\n",
    "I will begin addressing these factors by starting with factor 1. My approach will be to select a subset of the data that includes leukocite morphologies with roughly equal class counts. Then, I will test my model on this subset and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-accessory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
