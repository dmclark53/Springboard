{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "processed-angel",
   "metadata": {},
   "source": [
    "# Water Pumps: Modeling\n",
    "## Business Problem:\n",
    "Tanzania is a developing country and access to water is very important for the health of the population. For this reason, it is vital that all water pumps are properly working. Currently, the only way to monitor pump working status is by physically visiting the site. This is time consuming and costly. Therefore, a more intelligent solution to monitor water pump status is desirable.\n",
    "\n",
    "This project will address the following question: How can the government of Tanzania improve water pump maintenance by knowing the pump functional status in advance?\n",
    "\n",
    "**Goal:** The client would like to err on the side of predicting a pump is failing, when in fact it is functional. This means, reducing type two error for _non-functional_ and _functional needs repair_ classes. Therefore, the modeling strategy will focus on improving the recall metric, especially related to these two classes.\n",
    "\n",
    "**Strategy:**\n",
    "In this notebook I will explore different models and the best set of hyperparameters to use for hyperparameter tuning. I will use what I learn here to create a separate notebook with a modeling pipeline (Water_Pumps_Modeling_Pipeline.ipynb).\n",
    "    \n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "operating-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-sailing",
   "metadata": {},
   "source": [
    "## Load Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "western-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test():\n",
    "    file_list = ['X_train', 'X_test', 'y_train', 'y_test']\n",
    "    data_sets = []\n",
    "    for filename in file_list:\n",
    "        data_sets.append(pickle.load(open(f'../data/clean/{filename}', 'rb')))\n",
    "    return tuple(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adjustable-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-brooklyn",
   "metadata": {},
   "source": [
    "Load predictions from baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "israeli-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_base = pickle.load(open(f'../data/clean/y_test_base', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-responsibility",
   "metadata": {},
   "source": [
    "## Rescaling\n",
    "Rescale the features to values between 0 and 1. Since the categorical variables are one-hot-encoded, this will ensure that the continuous variables are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incident-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_rescaled = scaler.transform(X_train)\n",
    "X_test_rescaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-evening",
   "metadata": {},
   "source": [
    "Check the value range after rescaling the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assigned-magazine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rescaled.min(), X_train_rescaled.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-defensive",
   "metadata": {},
   "source": [
    "Check the value range after rescaling the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improved-slave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.388478523218112e-06, 1.0018616381450016)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rescaled.min(), X_test_rescaled.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-boulder",
   "metadata": {},
   "source": [
    "Check the shape of the training and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "yellow-element",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21296, 230), (9127, 230))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rescaled.shape, X_test_rescaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-router",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "Counter the class imbalanced data set by performing resampling. I will consider both over sampling and under sampling.\n",
    "\n",
    "### Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prompt-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over, y_train_over = SMOTE().fit_resample(X_train_rescaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powerful-pressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional needs repair    12482\n",
      "non functional             12482\n",
      "functional                 12482\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-inside",
   "metadata": {},
   "source": [
    "### Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "manual-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under, y_train_under = RandomUnderSampler(random_state=42).fit_resample(X_train_rescaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exact-fitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional                 1505\n",
      "non functional             1505\n",
      "functional needs repair    1505\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train_under).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-geneva",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "I will create two sets of models, one for over sampled training sets, and another for under sampled training sets. For each set of models, I will consider the following models:\n",
    "* Logistic Regression.\n",
    "* Random Forrest.\n",
    "* XGBoost.\n",
    "\n",
    "### Models with Over Sampling\n",
    "#### Logistic Regression\n",
    "First, I will try a basic logistic regression model, to get a base line for the over sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "postal-termination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', solver='saga')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_over = LogisticRegression(solver='saga', multi_class='multinomial')\n",
    "logreg_over.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compliant-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_logreg_over = logreg_over.predict(X_train_over)\n",
    "y_pred_logreg_over = logreg_over.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alleged-acquisition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.69      0.66      0.68     12482\n",
      "functional needs repair       0.69      0.77      0.73     12482\n",
      "         non functional       0.74      0.69      0.71     12482\n",
      "\n",
      "               accuracy                           0.70     37446\n",
      "              macro avg       0.71      0.70      0.70     37446\n",
      "           weighted avg       0.71      0.70      0.70     37446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_over, y_pred_train_logreg_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "seventh-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.84      0.66      0.74      5349\n",
      "functional needs repair       0.23      0.73      0.34       645\n",
      "         non functional       0.74      0.68      0.70      3133\n",
      "\n",
      "               accuracy                           0.67      9127\n",
      "              macro avg       0.60      0.69      0.60      9127\n",
      "           weighted avg       0.76      0.67      0.70      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_logreg_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nominated-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_train = classification_report(y_train_over, y_pred_train_logreg_over, output_dict=True)\n",
    "df_cr_train = pd.DataFrame(cr_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aging-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cr_train.drop(columns=['f1-score', 'support'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "expensive-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cr_train.drop(['accuracy', 'macro avg', 'weighted avg'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "terminal-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>functional</th>\n",
       "      <td>0.690854</td>\n",
       "      <td>0.662073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>functional needs repair</th>\n",
       "      <td>0.688775</td>\n",
       "      <td>0.765422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non functional</th>\n",
       "      <td>0.738224</td>\n",
       "      <td>0.686829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         precision    recall\n",
       "functional                0.690854  0.662073\n",
       "functional needs repair   0.688775  0.765422\n",
       "non functional            0.738224  0.686829"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "textile-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'logreg_over'\n",
    "df_new = df_cr_train.copy()\n",
    "multi_columns = [(model_type, x) for x in df_new.columns]\n",
    "df_new.columns = pd.MultiIndex.from_tuples(multi_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deluxe-sodium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">logreg_over</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>functional</th>\n",
       "      <td>0.690854</td>\n",
       "      <td>0.662073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>functional needs repair</th>\n",
       "      <td>0.688775</td>\n",
       "      <td>0.765422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non functional</th>\n",
       "      <td>0.738224</td>\n",
       "      <td>0.686829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        logreg_over          \n",
       "                          precision    recall\n",
       "functional                 0.690854  0.662073\n",
       "functional needs repair    0.688775  0.765422\n",
       "non functional             0.738224  0.686829"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-timeline",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "The roughly 5% drop in performance between the training and test scores indicates that over fitting could be a problem. I will try to address this using cross-validation. I will also use a grid search to identify the best regularization parameter, which could reduce the over fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "undefined-inquiry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=LogisticRegression(max_iter=10000,\n",
       "                                                multi_class='multinomial',\n",
       "                                                solver='saga'),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': array([0.2, 0.6, 1. , 1.4, 1.8, 2.2]),\n",
       "                                        'penalty': ['l1', 'l2']},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_over_rs = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000)\n",
    "rs_logreg_params = {'C': np.arange(0.2, 2.4, 0.4), 'penalty': ['l1', 'l2']}\n",
    "rs_logreg = RandomizedSearchCV(logreg_over_rs, rs_logreg_params, random_state=42, n_jobs=-1)\n",
    "rs_logreg.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "comprehensive-algeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best value for C is 2.200.\n",
      "The best penalty is l2.\n"
     ]
    }
   ],
   "source": [
    "best_C = rs_logreg.best_estimator_.get_params()['C']\n",
    "best_penalty = rs_logreg.best_estimator_.get_params()['penalty']\n",
    "print(f'The best value for C is {best_C:0.3f}.')\n",
    "print(f'The best penalty is {best_penalty}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "statistical-payroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.2000000000000006, max_iter=10000,\n",
       "                   multi_class='multinomial', solver='saga')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_over_2 = LogisticRegression(solver='saga', multi_class='multinomial', C=best_C, penalty=best_penalty, max_iter=10000)\n",
    "logreg_over_2.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "promising-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_logreg_over_2 = logreg_over_2.predict(X_train_over)\n",
    "y_pred_logreg_over_2 = logreg_over_2.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dress-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.69      0.66      0.68     12482\n",
      "functional needs repair       0.69      0.77      0.73     12482\n",
      "         non functional       0.74      0.69      0.71     12482\n",
      "\n",
      "               accuracy                           0.71     37446\n",
      "              macro avg       0.71      0.71      0.71     37446\n",
      "           weighted avg       0.71      0.71      0.71     37446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_over, y_pred_train_logreg_over_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "disciplinary-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.85      0.66      0.74      5349\n",
      "functional needs repair       0.23      0.73      0.35       645\n",
      "         non functional       0.73      0.68      0.70      3133\n",
      "\n",
      "               accuracy                           0.67      9127\n",
      "              macro avg       0.60      0.69      0.60      9127\n",
      "           weighted avg       0.76      0.67      0.70      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_logreg_over_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-beijing",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "After searching for better parameters using a randomized search, I do not see an improvement in the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-local",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Next, I will try random forest. As with the logistic regression model, I will fit a model with no hyperparameter tuning to get a base line model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ideal-cleanup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_over = RandomForestClassifier(n_estimators=100, random_state = 42, n_jobs=-1)\n",
    "rf_over.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "focal-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_over = rf_over.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "potential-contrary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.83      0.86      0.85      5349\n",
      "functional needs repair       0.46      0.42      0.44       645\n",
      "         non functional       0.81      0.77      0.79      3133\n",
      "\n",
      "               accuracy                           0.80      9127\n",
      "              macro avg       0.70      0.69      0.69      9127\n",
      "           weighted avg       0.80      0.80      0.80      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-prize",
   "metadata": {},
   "source": [
    "**Observations:** Without doing any hyperparameter tuning, we can see mixed results as compared with the logistic regression model. The minority class shows an improvement in precision, but a decrease in recall. On the otherhand, the majority class shows an improvement in recall for the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "third-contest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='sqrt', n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_over_2 = RandomForestClassifier(n_estimators=100, random_state = 42, max_features='sqrt', n_jobs=-1)\n",
    "rf_over_2.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "talented-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_over_2 = rf_over_2.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acquired-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.83      0.86      0.85      5349\n",
      "functional needs repair       0.46      0.42      0.44       645\n",
      "         non functional       0.81      0.77      0.79      3133\n",
      "\n",
      "               accuracy                           0.80      9127\n",
      "              macro avg       0.70      0.69      0.69      9127\n",
      "           weighted avg       0.80      0.80      0.80      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf_over_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "furnished-replacement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='sqrt', n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_over_3 = RandomForestClassifier(n_estimators=100, random_state = 42, max_features='sqrt', max_depth=None, min_samples_split=2, n_jobs=-1)\n",
    "rf_over_3.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interpreted-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_over_3 = rf_over_3.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "regulation-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.83      0.86      0.85      5349\n",
      "functional needs repair       0.46      0.42      0.44       645\n",
      "         non functional       0.81      0.77      0.79      3133\n",
      "\n",
      "               accuracy                           0.80      9127\n",
      "              macro avg       0.70      0.69      0.69      9127\n",
      "           weighted avg       0.80      0.80      0.80      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf_over_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-parent",
   "metadata": {},
   "source": [
    "**Observations:** After adjusting the parameters using the [suggestions](https://scikit-learn.org/stable/modules/ensemble.html#random-forest-parameters) from scikit-learn, there is still not an improvement in recall for the minority classes. I will need to do a grid search with cross validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pleased-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_over_rs = RandomForestClassifier(random_state = 42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "honest-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_list = list(np.arange(10, 110, 10))\n",
    "max_depth_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "exact-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_params_rf_over = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': max_depth_list,\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': list(np.arange(1, 11, 1)),\n",
    "    'min_samples_split': list(np.arange(1, 11, 1)),\n",
    "    'n_estimators': list(np.arange(200, 2200, 200))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "overall-positive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestClassifier(n_jobs=-1, random_state=42),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5, 6,\n",
       "                                                             7, 8, 9, 10],\n",
       "                                        'min_samples_split': [1, 2, 3, 4, 5, 6,\n",
       "                                                              7, 8, 9, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_rf_over = RandomizedSearchCV(rf_over_rs, rs_params_rf_over, random_state=42, n_jobs=-1)\n",
    "rs_rf_over.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "satisfied-woman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 800,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 90,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_rf_over.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "frequent-playing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=90, min_samples_split=3, n_estimators=1600,\n",
       "                       n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_over_4 = RandomForestClassifier(n_estimators=1600, \n",
    "                                   random_state = 42, \n",
    "                                   max_features='auto', \n",
    "                                   max_depth=90, \n",
    "                                   min_samples_split=3, \n",
    "                                   min_samples_leaf=1,\n",
    "                                   bootstrap=True,\n",
    "                                   n_jobs=-1)\n",
    "rf_over_4.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "least-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_over_4 = rf_over_4.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "false-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.84      0.86      0.85      5349\n",
      "functional needs repair       0.47      0.42      0.44       645\n",
      "         non functional       0.81      0.78      0.79      3133\n",
      "\n",
      "               accuracy                           0.80      9127\n",
      "              macro avg       0.70      0.69      0.70      9127\n",
      "           weighted avg       0.80      0.80      0.80      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_rf_over_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-minimum",
   "metadata": {},
   "source": [
    "**Observations:** I do not see any large improvement in metrics, recall nor precision, after tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-madonna",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "I will now try an XGBoost algorithm with the over sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "stunning-opportunity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['functional', 'functional needs repair', 'non functional'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corresponding-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    'functional': 0,\n",
    "    'functional needs repair': 1,\n",
    "    'non functional': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_over_encoded = pd.Series(y_train_over).replace(class_mapping).values\n",
    "y_test_encoded = pd.Series(y_test).replace(class_mapping).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "appreciated-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:05:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=16, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_over_1 = XGBClassifier()\n",
    "xgb_over_1.fit(X_train_over, y_train_over_encoded)\n",
    "print(xgb_over_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "architectural-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_over = xgb_over_1.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "helpful-garlic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83      5349\n",
      "           1       0.38      0.47      0.42       645\n",
      "           2       0.81      0.73      0.77      3133\n",
      "\n",
      "    accuracy                           0.78      9127\n",
      "   macro avg       0.67      0.68      0.67      9127\n",
      "weighted avg       0.79      0.78      0.78      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_pred_xgb_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-franklin",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "The XGBoost algorithm performs slightly better in recall on the minority class, but slightly worse in precision. Precision and recall is worse for the additional classes.\n",
    "\n",
    "Next, I will try two sets of hyperparameters suggested by a couple of blog posts. The first set of hyperparameters are taken from [here](https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "respective-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:50:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=1, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=1000, n_jobs=16, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_over_2 = XGBClassifier(\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=1000,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=1,\n",
    "    gamma=1\n",
    ")\n",
    "xgb_over_2.fit(X_train_over, y_train_over_encoded)\n",
    "print(xgb_over_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "super-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_over_2 = xgb_over_2.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "encouraging-leeds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77      5349\n",
      "           1       0.26      0.58      0.36       645\n",
      "           2       0.72      0.68      0.70      3133\n",
      "\n",
      "    accuracy                           0.70      9127\n",
      "   macro avg       0.60      0.66      0.61      9127\n",
      "weighted avg       0.74      0.70      0.72      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_pred_xgb_over_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-secretary",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "The recall has improved for the minority classes.\n",
    "\n",
    "Now, I will try a different set of initial hyperparameters suggested in this [article](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "about-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:59:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:59:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=1000, n_jobs=16, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_over_3 = XGBClassifier(\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    n_estimators=1000\n",
    ")\n",
    "xgb_over_3.fit(X_train_over, y_train_over_encoded)\n",
    "print(xgb_over_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "uniform-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_over_3 = xgb_over_3.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "representative-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84      5349\n",
      "           1       0.46      0.38      0.42       645\n",
      "           2       0.79      0.77      0.78      3133\n",
      "\n",
      "    accuracy                           0.80      9127\n",
      "   macro avg       0.69      0.67      0.68      9127\n",
      "weighted avg       0.79      0.80      0.79      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_pred_xgb_over_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-price",
   "metadata": {},
   "source": [
    "**Observations:** The recall actually performed worse for the minority class. But, the recall for the _non functional_ class performed better.\n",
    "\n",
    "##### Hyperparameter Tuning\n",
    "After trying a few sets of initial hyperparameter values, it is time to tune them to find the optimal set.\n",
    "\n",
    "The low recall score could be due to over fitting. The XGBoost documentation suggests that over fitting can be reduced by optimizing the hyperparameters _max_depth_, _min_child_weight_ and _gamma_.\n",
    "\n",
    "First, I will optimize _max_depth_ and _min_child_weight_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "neural-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_params_xgb_over = {\n",
    "    'max_depth': list(np.arange(1, 7, 2)),\n",
    "    'min_child_weight': list(np.arange(1, 7, 2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "natural-education",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 9 is smaller than n_iter=100. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'min_child_weight': 1, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "xgb_over_4 = XGBClassifier(n_estimators=1000)\n",
    "rs_xgb_over_1 = RandomizedSearchCV(xgb_over_4, rs_params_xgb_over, random_state=42, n_jobs=-1, n_iter=100)\n",
    "rs_xgb_over_1.fit(X_train_over, y_train_over_encoded)\n",
    "print(rs_xgb_over_1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-nurse",
   "metadata": {},
   "source": [
    "Next, I will optimize _gamma_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lesbian-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_params_xgb_over_2 = {\n",
    "    'gamma': [0, 1, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "indoor-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:27:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "xgb_over_5 = XGBClassifier(n_estimators=1000, min_child_weight=1, max_depth=5)\n",
    "rs_xgb_over_2 = RandomizedSearchCV(xgb_over_5, rs_params_xgb_over_2, random_state=42, n_jobs=-1, n_iter=100)\n",
    "rs_xgb_over_2.fit(X_train_over, y_train_over_encoded)\n",
    "print(rs_xgb_over_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "operating-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:19:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=1000, n_jobs=16, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_over_6 = XGBClassifier(n_estimators=1000, min_child_weight=1, max_depth=5, gamma=0)\n",
    "xgb_over_6.fit(X_train_over, y_train_over_encoded)\n",
    "print(xgb_over_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unusual-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_over_6 = xgb_over_6.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "swedish-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85      5349\n",
      "           1       0.46      0.37      0.41       645\n",
      "           2       0.81      0.77      0.79      3133\n",
      "\n",
      "    accuracy                           0.80      9127\n",
      "   macro avg       0.70      0.67      0.68      9127\n",
      "weighted avg       0.80      0.80      0.80      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_pred_xgb_over_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-croatia",
   "metadata": {},
   "source": [
    "Finally, I will increase the number of trees and lower the learning rate, along with using the best hyperparameter values I found for _max_depth_, _min_child_weight_ and _gamma_. This should improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acting-opposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dave/anaconda3/envs/springboard/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:52:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.01, max_delta_step=0, max_depth=5,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=5000, n_jobs=16, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "xgb_over_7 = XGBClassifier(n_estimators=5000, min_child_weight=1, max_depth=5, gamma=0, learning_rate=0.01)\n",
    "xgb_over_7.fit(X_train_over, y_train_over_encoded)\n",
    "print(xgb_over_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "swedish-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_over_7 = xgb_over_7.predict(X_test_rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "mental-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      5349\n",
      "           1       0.39      0.47      0.43       645\n",
      "           2       0.81      0.74      0.77      3133\n",
      "\n",
      "    accuracy                           0.78      9127\n",
      "   macro avg       0.67      0.68      0.68      9127\n",
      "weighted avg       0.79      0.78      0.78      9127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoded, y_pred_xgb_over_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-value",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "In this notebook, I explored three different models to predict water pump functional status:\n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "For each model, I performed hyper parameter tuning. I determined which hyperparameter to tune based on whether they were effective at reducing overfitting and based on the recommendations of the Scikit-Learn documentation. This notebook only focused on over sampling the data and served to refine my modeling strategy.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "I will used what I learned here to develope a pipeline in a separate notebook. This pipeline will be used to model over and under sampled data as well as converting modeling strategy to a binary classification problem. The final results and conclusions will be included in that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-memorial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
